{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c7d0ac",
   "metadata": {},
   "source": [
    "<img src=\"img/marcov.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28fa06",
   "metadata": {},
   "source": [
    "Going from state St to state St + 1 requires only state St, not previous states, otherwise it is not a marcov."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668cfcf1",
   "metadata": {},
   "source": [
    "<img src=\"img/Transition_step.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775795b",
   "metadata": {},
   "source": [
    "## The Reward is not importand in State Transition Function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bc4b07",
   "metadata": {},
   "source": [
    "<img src=\"img/State_Transition_Function.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e171e",
   "metadata": {},
   "source": [
    "# Reward Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d8d2a",
   "metadata": {},
   "source": [
    "<img src=\"img/Reward_Function.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e4a60c",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747fad69",
   "metadata": {},
   "source": [
    "1. Deterministic: π(st) = a\n",
    "2. Stochastic (Probabilistic): π(a|s) = Pπ[A=a, S=s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c168de4b",
   "metadata": {},
   "source": [
    "# Return\n",
    "Gt is: Predict future rewards in each state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5884ae14",
   "metadata": {},
   "source": [
    "<img src=\"img/return.png\">\n",
    "<h2>Example</h2>\n",
    "<img src=\"img/return_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8021c6db",
   "metadata": {},
   "source": [
    "# Discounting Factor (why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d62cae6",
   "metadata": {},
   "source": [
    "It is used for two reasons that it is a value in the range of 0-1:\n",
    "1. Reduce the impact of future rewards versus current rewards\n",
    "2. There may be an infinite state in the Return function and its definition is simple.\n",
    "<img src=\"img/discounting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e600d9",
   "metadata": {},
   "source": [
    "# State-Value\n",
    "Specifies the value of each state according to the defined policy.\n",
    "Expected all Gis in state St\n",
    "<img src=\"img/state-value.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63a5e0",
   "metadata": {},
   "source": [
    "# Action-Value\n",
    "Expected all Gis after do action At in state St \n",
    "<img src=\"img/action-value.png\">\n",
    "<img src=\"img/action_value_ex.png\">\n",
    "<img src=\"img/action_alue_ex2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc0d9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
