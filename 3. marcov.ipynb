{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c7d0ac",
   "metadata": {},
   "source": [
    "<img src=\"img/marcov.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28fa06",
   "metadata": {},
   "source": [
    "Going from state St to state St + 1 requires only state St, not previous states, otherwise it is not a marcov."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668cfcf1",
   "metadata": {},
   "source": [
    "<img src=\"img/Transition_step.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775795b",
   "metadata": {},
   "source": [
    "## The Reward is not importand in State Transition Function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bc4b07",
   "metadata": {},
   "source": [
    "<img src=\"img/State_Transition_Function.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e171e",
   "metadata": {},
   "source": [
    "# Reward Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d8d2a",
   "metadata": {},
   "source": [
    "<img src=\"img/Reward_Function.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e4a60c",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747fad69",
   "metadata": {},
   "source": [
    "1. Deterministic: π(st) = a\n",
    "2. Stochastic (Probabilistic): π(a|s) = Pπ[A=a, S=s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c168de4b",
   "metadata": {},
   "source": [
    "# Return\n",
    "Gt is: Predict future rewards in each state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5884ae14",
   "metadata": {},
   "source": [
    "<img src=\"img/return.png\">\n",
    "<h2>Example</h2>\n",
    "<img src=\"img/return_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8021c6db",
   "metadata": {},
   "source": [
    "# Discounting Factor (why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d62cae6",
   "metadata": {},
   "source": [
    "It is used for two reasons that it is a value in the range of 0-1:\n",
    "1. Reduce the impact of future rewards versus current rewards\n",
    "2. There may be an infinite state in the Return function and its definition is simple.\n",
    "<img src=\"img/discounting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e600d9",
   "metadata": {},
   "source": [
    "# State-Value\n",
    "Specifies the value of each state according to the defined policy.\n",
    "Expected all Gis in state St\n",
    "<img src=\"img/state-value.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63a5e0",
   "metadata": {},
   "source": [
    "# Action-Value\n",
    "Expected all Gis after do action At in state St \n",
    "<img src=\"img/action-value.png\">\n",
    "<img src=\"img/action_value_ex.png\">\n",
    "<img src=\"img/action_alue_ex2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cf6c95",
   "metadata": {},
   "source": [
    "# Best Police\n",
    "The policy that estimates the most value in state s is the best policy. Q<sub>*</sub>(s,a)<br>\n",
    "This value is called the best state s value. V<sub>*</sub>(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea56aad",
   "metadata": {},
   "source": [
    "<img src=\"img/best_police.png\">\n",
    "<h2 style=\"text-align:center\">Can all policies be reviewed? <br>No</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b93b3c",
   "metadata": {},
   "source": [
    "# Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b02a6e8",
   "metadata": {},
   "source": [
    "<img src=\"img/bellman-eq.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2791c0",
   "metadata": {},
   "source": [
    "<img src=\"img/belman-eq-2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b7b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "باید امید ریاضی اکسپوننشیال قدر ایکس به توان تتا تقسیم بر سی به توان تتا رو حساب کنه"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0278e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def E(x, t, c):\n",
    "    r = np.exp(np.abs((x**t)/(c**t))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91b3a5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8452/305861849.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  r = np.exp((np.absolute(MI) ** theta)/(j**theta))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.77"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def norm(MI,theta):\n",
    "    for j in np.arange(1,1000.)/100:\n",
    "        r = np.exp((np.absolute(MI) ** theta)/(j**theta))\n",
    "        E = np.mean(r)\n",
    "#         print(r)\n",
    "        if E<=2:\n",
    "            return j\n",
    "    return 0\n",
    "norm(np.array(range(1,100))/100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab5af1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11,\n",
       "       0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22,\n",
       "       0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33,\n",
       "       0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44,\n",
       "       0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54, 0.55,\n",
       "       0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66,\n",
       "       0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77,\n",
       "       0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88,\n",
       "       0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(range(1,100))/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e3271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(MI,theta):\n",
    "  for j in np.arange(0,1000.)/100:\n",
    "    E=np.mean(np.exp((np.absolute(MI) ** theta)/(j****theta)))\n",
    "    if E<=2:\n",
    "      return j\n",
    "  return 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
